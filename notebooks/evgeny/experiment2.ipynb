{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7030819b",
   "metadata": {},
   "source": [
    "## NN Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4430b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67746950",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_PATHS = [\"..\"]\n",
    "import sys\n",
    "for path in PYTHON_PATHS:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from classifier.resnet18 import Resnet18Model\n",
    "from data_loader.orb_features_data_module import ORBFeaturesDataModule\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875d433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/alena/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = Resnet18Model(nn.Sequential(nn.Linear(512, 512),\n",
    "                                           nn.Linear(512, 512),\n",
    "                                           nn.Linear(512, 2)\n",
    "                                          ),\n",
    "                             freeze_backbone=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5612bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_module_params = {'root': \"/home/alena/Documents/omni-vision/dataset\", \n",
    "                  'dataset': [\"dataset_2_cam_0\",\n",
    "                              \"dataset_2_cam_1\",\n",
    "                              \"dataset_2_cam_2\",\n",
    "                              \"dataset_2_cam_3\",\n",
    "                              \"dataset_2_cam_5\",\n",
    "                              \"dataset_3_cam_0\",\n",
    "                              \"dataset_3_cam_1\",\n",
    "                              \"dataset_3_cam_2\",\n",
    "                              \"dataset_3_cam_3\",\n",
    "                              \"dataset_3_cam_5\"\n",
    "                             ],\n",
    "                  'classification_threshold': 300,\n",
    "                  'transform': model.transform,\n",
    "                  'batch_size': 128+32,\n",
    "                  'shuffle': True,\n",
    "                  'num_workers': 6,\n",
    "                  'num_val': 1000,\n",
    "                  \"num_test\": 1000,}\n",
    "\n",
    "datamodule = ORBFeaturesDataModule(**dataset_module_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasetname: dataset_2_cam_0\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_2_cam_1\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_2_cam_2\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_2_cam_3\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_2_cam_5\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_3_cam_0\n",
      "num images: 6376\n",
      "num labels: 6376\n",
      "datasetname: dataset_3_cam_1\n",
      "num images: 6376\n",
      "num labels: 6376\n",
      "datasetname: dataset_3_cam_2\n",
      "num images: 6376\n",
      "num labels: 6376\n",
      "datasetname: dataset_3_cam_3\n",
      "num images: 6376\n",
      "num labels: 6376\n",
      "datasetname: dataset_3_cam_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num images: 6376\n",
      "num labels: 6376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/alena/repos/my_orb/orb-classification/notebooks/lightning_logs\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | activation | LogSoftmax | 0     \n",
      "1 | loss       | NLLLoss    | 0     \n",
      "2 | backbone   | ResNet     | 11.2 M\n",
      "3 | fc         | Sequential | 526 K \n",
      "------------------------------------------\n",
      "11.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.7 M    Total params\n",
      "46.811    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alena/miniconda3/envs/omni-vision/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:659: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25043ab90ca144c7bb75a3c9f5c4d751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, log_every_n_steps=1, max_epochs=8)\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2664f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from data_loader.orb_features_dataset import ORBFeaturesDataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3b2434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasetname: dataset_2_cam_0\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_2_cam_1\n",
      "num images: 2546\n",
      "num labels: 2546\n",
      "datasetname: dataset_2_cam_2\n",
      "num images: 2546\n",
      "num labels: 2546\n"
     ]
    }
   ],
   "source": [
    "dataset_params = {'root': \"/home/alena/Documents/omni-vision/dataset\", \n",
    "                  'datasets': [\n",
    "#                               \"dataset_1_cam_0\",\n",
    "                              \"dataset_1_cam_1\",\n",
    "                              \"dataset_1_cam_2\",\n",
    "                              \"dataset_1_cam_3\",\n",
    "                              \"dataset_1_cam_5\"\n",
    "                             ],\n",
    "                  'classification_threshold': 300,\n",
    "                  'transform': model.transform,\n",
    "                  'combine_data': False\n",
    "                 }\n",
    "\n",
    "dataset = ORBFeaturesDataset(**dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c91107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/alena/Documents/omni-vision/dataset/dataset_2_cam_0/images/000000.png',\n",
       "       '/home/alena/Documents/omni-vision/dataset/dataset_2_cam_0/images/000001.png',\n",
       "       '/home/alena/Documents/omni-vision/dataset/dataset_2_cam_0/images/000002.png',\n",
       "       ...,\n",
       "       '/home/alena/Documents/omni-vision/dataset/dataset_2_cam_0/images/002543.png',\n",
       "       '/home/alena/Documents/omni-vision/dataset/dataset_2_cam_0/images/002544.png',\n",
       "       '/home/alena/Documents/omni-vision/dataset/dataset_2_cam_0/images/002545.png'],\n",
       "      dtype='<U75')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.images_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5b8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_img(img_path, tranform):\n",
    "    image = read_image(img_path)[None, ...]\n",
    "    return tranform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb2ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3a1f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0720c36ef5fa4b3ebda8691b0eee175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for d in tqdm(dataset.images_list):\n",
    "    preds= []\n",
    "    \n",
    "    for img_path in d:\n",
    "        img = read_dataset_img(img_path, model.transform)\n",
    "        preds.append(torch.nn.functional.softmax(model.forward(img), dim=1).detach().cpu().numpy())\n",
    "        \n",
    "    predictions.append(preds)\n",
    "    \n",
    "labels = dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317a3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for p in predictions:\n",
    "    pred_proba = np.squeeze(np.array(p), axis=1)\n",
    "    pred.append(np.argmax(pred_proba, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068da14a",
   "metadata": {},
   "source": [
    "## Camera Switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6f49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d04fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.5123024e-12, 1.3130646e-11, 4.2012596e-12, 1.8361937e-11,\n",
       "       4.8318355e-10], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(np.array(predictions[0][:5]), axis=1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e098ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argpartition(np.squeeze(np.array(predictions[0][:5]), axis=1)[:, 1], -2)[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58cefe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def choose_two_best(valid_cams, cams_proba, num_cams=2):\n",
    "#     if sum(valid_cams) == num_cams:\n",
    "#         idx = np.where(valid_cams == True)\n",
    "#         return idx[0], idx[1]\n",
    "    \n",
    "#     elif sum(valid_cams) > num_cams:\n",
    "#         # find indexes of two maximal values in probabilities class 1\n",
    "#         idx = np.argpartition(cam_proba[:, 1], -num_cams)[-num_cams:]\n",
    "#         return idx[0], idx[1]\n",
    "    \n",
    "#     elif sum(valid_cams) < 2:\n",
    "#         # find indexes of two minims values in probabilities class 0\n",
    "#         idx = np.argpartition(cam_proba[:, 1], -2)[-2:]\n",
    "#         return idx[0], idx[1]\n",
    "        \n",
    "\n",
    "def get_valid_cams(idx, pred, pred_proba, num_cams=2):\n",
    "    valid_cams = [p[idx] for p in pred]\n",
    "    cams_proba = np.squeeze([p[idx] for p in pred_proba], axis=1)\n",
    "    \n",
    "#     print(cams_proba)\n",
    "    cam_ids = np.argpartition(cams_proba[:, 1], -2)[-2:]\n",
    "\n",
    "    return cam_ids[0], cam_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef523e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pred[0].shape[0]\n",
    "cameras = []\n",
    "\n",
    "for i in range(N):\n",
    "    cam1, cam2 = get_valid_cams(i, pred, predictions)\n",
    "    cameras.append([cam1, cam2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f39ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcameras\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "dataset.features_list[cameras[0], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam1_data = []\n",
    "cam2_data = []\n",
    "\n",
    "for i in range(len(cameras)):\n",
    "    cam1_data.append(dataset.features_list[cameras[i][0]][i])\n",
    "    cam2_data.append(dataset.features_list[cameras[i][1]][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e03bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2282673",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "PYTHON_PATHS = [\"../orb-classification/\"]\n",
    "import sys\n",
    "for path in PYTHON_PATHS:\n",
    "    if path not in sys.path:\n",
    "        sys.path.append(path)\n",
    "\n",
    "from data_loader.orb_features_dataset import ORBFeaturesDataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_params = {'root': \"/home/alena/Documents/omni-vision/dataset\", \n",
    "#                   'datasets': [\n",
    "#                               \"dataset_2_cam_0\",\n",
    "#                               \"dataset_2_cam_1\",\n",
    "#                               \"dataset_2_cam_2\",\n",
    "#                              #\"dataset_1_cam_0\",\n",
    "#                              ],\n",
    "#                   'classification_threshold': 300,\n",
    "#                   'transform': None,\n",
    "#                   'combine_data': False\n",
    "#                  }\n",
    "\n",
    "# dataset = ORBFeaturesDataset(**dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d762a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset.labels)):\n",
    "    plt.subplots(figsize=(8, 2))\n",
    "    plt.title(f\"Number of ORB features on sequence{0}, camera{i}\")\n",
    "    plt.xlabel(\"frame num\")\n",
    "    plt.ylabel(\"num features\")\n",
    "    plt.plot(dataset.features_list[i])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_mean_coeff = 20\n",
    "\n",
    "def running_mean(x, N=running_mean_coeff):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "cam_features = []\n",
    "for i in range(len(dataset.labels)):\n",
    "    cam_features.append(running_mean(dataset.features_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset.labels)):\n",
    "    plt.subplots(figsize=(8, 2))\n",
    "    plt.title(f\"Number of ORB features on sequence{0}, camera{i}\")\n",
    "    plt.xlabel(\"frame num\")\n",
    "    plt.ylabel(\"num features\")\n",
    "    plt.plot(dataset.features_list[i])\n",
    "    plt.plot(cam_features[i])\n",
    "    plt.show();\n",
    "\n",
    "    \n",
    "cam2_data_s = running_mean(cam2_data)\n",
    "plt.subplots(figsize=(8, 2))\n",
    "plt.title(f\"Our approach with camera switch\")\n",
    "plt.xlabel(\"frame num\")\n",
    "plt.ylabel(\"num features\")\n",
    "plt.plot(cam2_data)\n",
    "plt.plot(cam2_data_s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16, 6))\n",
    "plt.title(f\"Number of ORB features on sequence{0}, camera{i}\")\n",
    "plt.xlabel(\"frame num\")\n",
    "plt.ylabel(\"num features\")\n",
    "plt.plot(cam_features[0])\n",
    "plt.plot(cam_features[1])\n",
    "plt.plot(cam_features[2])\n",
    "plt.plot(cam2_data_s, linewidth=3, color='k')\n",
    "plt.legend([\"cam0\", \"cam1\", \"cam2\", \"ours\"])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c='rgbyk'\n",
    "for i in range(len(dataset.labels)):\n",
    "    plt.subplots(figsize=(8, 2))\n",
    "    plt.title(f\"Number of ORB features on sequence{0}, camera{i}\")\n",
    "    plt.xlabel(\"frame num\")\n",
    "    plt.ylabel(\"num features\")\n",
    "    plt.plot(cam_features[i], color=c[i])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3443fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cam in cameras:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "dataset_size = len(dataset.labels)\n",
    "# x = np.linspace(0, dataset_size, dataset_size)\n",
    "# y = dataset.features_list[0]\n",
    "\n",
    "for i in range(dataset_size):\n",
    "    fig = go.Figure()\n",
    "    y = dataset.features_list[i][:500]\n",
    "    x = np.linspace(0, y.shape[0], y.shape[0])\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=x, y=y))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc792b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
